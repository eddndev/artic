# Implementaci√≥n del Lexer de Artic

## ‚úÖ Estado: COMPLETADO

La implementaci√≥n del Lexer (tokenizador) para el lenguaje Artic ha sido completada exitosamente.

---

## üìÅ Archivos Creados

### Headers (.h)
1. **Token.h** - Definici√≥n de tipos de token y estructura Token
2. **SourceLocation.h** - Tracking de posici√≥n en el c√≥digo fuente
3. **Lexer.h** - Interfaz del lexer

### Implementation (.cpp)
1. **Token.cpp** - Implementaci√≥n de m√©todos de Token
2. **Lexer.cpp** - Implementaci√≥n completa del tokenizador

### Tests
1. **tests/unit/lexer/LexerTest.cpp** - Suite de tests manuales

### CLI
1. **src/cli/main.cpp** - Programa de l√≠nea de comandos para probar el lexer

---

## üéØ Caracter√≠sticas Implementadas

### Tipos de Token Soportados

#### Keywords (Palabras Clave)
- `use` - Para imports
- `from` - Para especificar origen de imports
- `theme` - Para importar temas
- `props` - Para definir propiedades de componentes
- `export` - Para exportar utilidades

#### Decoradores
- `@route` - Define rutas de p√°gina
- `@layout` - Define layout de p√°gina
- `@utility` - Define utilidades de estilo
- `@server` - Marca funciones de servidor (para fases futuras)
- `@client` - Marca funciones de cliente (para fases futuras)
- `@effect` - Marca efectos (para fases futuras)

#### Literales
- **Identificadores**: `count`, `userName`, `_private`, `$state`
- **Strings**: `"hello"`, `'world'`, con escape sequences
- **Numbers**: `42`, `3.14`
- **Booleans**: `true`, `false`
- **Null**: `null`, `undefined`

#### Operadores
- `=` - Asignaci√≥n/atributo HTML
- `:` - Directiva del compilador
- `=>` - Arrow function
- `?` - Ternario
- `|` - Pipe/Union type
- `.` - Acceso a miembro
- `+`, `-`, `*`, `/` - Aritm√©ticos
- `!` - Negaci√≥n

#### Delimitadores
- `(`, `)` - Par√©ntesis
- `{`, `}` - Llaves
- `[`, `]` - Corchetes
- `,` - Coma
- `;` - Punto y coma

#### Tokens HTML/Template
- `<` - Apertura de tag
- `>` - Cierre de tag
- `</` - Apertura de closing tag
- `/>` - Self-closing tag

#### Especiales
- `NEWLINE` - Saltos de l√≠nea
- `COMMENT` - Comentarios (filtrados en output)
- `END_OF_FILE` - Fin de archivo
- `ERROR` - Token de error

---

## üí° Funcionalidades del Lexer

### 1. Tokenizaci√≥n Completa
```cpp
Lexer lexer(source_code);
std::vector<Token> tokens = lexer.tokenize();
```

### 2. Tracking de Posici√≥n
Cada token guarda:
- N√∫mero de l√≠nea (1-indexed)
- N√∫mero de columna (1-indexed)
- Offset absoluto

### 3. Manejo de Comentarios
- Comentarios de l√≠nea: `// comment`
- Comentarios de bloque: `/* comment */`
- Se filtran autom√°ticamente del output

### 4. Strings con Escape Sequences
Soporta:
- `\"` - Comillas escapadas
- `\n` - Nueva l√≠nea
- `\t` - Tab
- `\r` - Carriage return
- `\\` - Backslash

### 5. Detecci√≥n de Errores
- Strings sin cerrar
- Caracteres desconocidos
- Decoradores inv√°lidos

---

## üìù Ejemplos de Uso

### Ejemplo 1: Tokenizar Decoradores
```cpp
Lexer lexer("@route(\"/hello\")");
auto tokens = lexer.tokenize();

// tokens[0] = AT_ROUTE("@route")
// tokens[1] = LPAREN("(")
// tokens[2] = STRING("/hello")
// tokens[3] = RPAREN(")")
// tokens[4] = END_OF_FILE
```

### Ejemplo 2: Tokenizar Utilidades
```cpp
Lexer lexer(R"(
    @utility
    btn {
        px:4 py:2
    }
)");
auto tokens = lexer.tokenize();

// Encuentra: @utility, btn, {, px, :, 4, py, :, 2, }
```

### Ejemplo 3: Tokenizar Template
```cpp
Lexer lexer("<div class:(container)>Hello</div>");
auto tokens = lexer.tokenize();

// Encuentra: <, div, class, :, (, container, ), >, Hello, </, div, >
```

---

## üß™ Tests Implementados

### Test Suite (tests/unit/lexer/LexerTest.cpp)

1. **testBasicTokens()** - Tokens b√°sicos de delimitadores
2. **testKeywords()** - Palabras clave
3. **testDecorators()** - Decoradores (@route, @utility, etc.)
4. **testIdentifiers()** - Identificadores y nombres de variables
5. **testStrings()** - Strings literals con comillas
6. **testNumbers()** - N√∫meros enteros y decimales
7. **testComments()** - Comentarios de l√≠nea y bloque
8. **testCompleteExample()** - Ejemplo completo de c√≥digo Artic
9. **testLineAndColumn()** - Tracking correcto de posici√≥n

### Ejecutar Tests
```bash
# Con CMake (cuando est√© configurado)
./build/artic_tests

# Manual
g++ -std=c++20 -I. src/frontend/lexer/Token.cpp src/frontend/lexer/Lexer.cpp tests/unit/lexer/LexerTest.cpp -o lexer_test
./lexer_test
```

---

## üîß CLI Tool

### Comando `lex`
```bash
artic lex <file.atc>
```

Tokeniza un archivo y muestra todos los tokens encontrados con su tipo, lexema y posici√≥n.

**Ejemplo:**
```bash
artic lex examples/hello_world/index.atc
```

**Output:**
```
Tokenizing: examples/hello_world/index.atc

Found 42 tokens:

AT_ROUTE('@route') [1:1]
LPAREN('(') [1:7]
STRING('/') [1:8]
RPAREN(')') [1:11]
NEWLINE('\n') [1:12]
...
END_OF_FILE('') [15:1]
```

---

## üìä Estad√≠sticas de Implementaci√≥n

### L√≠neas de C√≥digo
- **Token.h**: ~120 l√≠neas
- **Token.cpp**: ~80 l√≠neas
- **SourceLocation.h**: ~30 l√≠neas
- **Lexer.h**: ~120 l√≠neas
- **Lexer.cpp**: ~350 l√≠neas
- **LexerTest.cpp**: ~230 l√≠neas

**Total: ~930 l√≠neas de c√≥digo**

### Tipos de Token: 45 tipos definidos

### M√©todos del Lexer:
- P√∫blicos: 3 m√©todos
- Privados: 13 m√©todos auxiliares

---

## üéØ Casos de Uso Soportados (Fase 1)

El Lexer est√° optimizado para la **Fase 1: Generaci√≥n de Sitios Est√°ticos (SSG)**.

### Sintaxis Soportada:

#### 1. Metadata
```artic
@route("/dashboard")
@layout("admin")
```

#### 2. Imports
```artic
use theme "./theme.atc"
use { Card, Button } from "./components"
```

#### 3. Utilidades de Estilo
```artic
@utility
btn {
    px:4 py:2
    bg:blue-500
    hover:bg:blue-700
}
```

#### 4. Templates HTML
```artic
<div class:(container)>
    <h1>Hello World</h1>
</div>
```

---

## ‚úÖ Validaci√≥n

### Compilaci√≥n
El c√≥digo ha sido verificado para compilar correctamente con:
- **Est√°ndar**: C++20
- **Compiladores**: GCC 15.1.0+, Clang, MSVC
- **Warnings**: `-Wall -Wextra -Wpedantic`

### Tests
Todos los tests manuales pasan correctamente:
- ‚úÖ Tokens b√°sicos
- ‚úÖ Keywords
- ‚úÖ Decoradores
- ‚úÖ Identificadores
- ‚úÖ Strings
- ‚úÖ Numbers
- ‚úÖ Comentarios
- ‚úÖ Ejemplo completo
- ‚úÖ Tracking de posici√≥n

---

## üöÄ Pr√≥ximos Pasos

Con el Lexer completado, los siguientes componentes a implementar son:

### Fase 1 (SSG) - Continuaci√≥n

1. **Parser** (Prioridad 1)
   - Construir AST desde tokens
   - Parser recursivo descendente
   - Validaci√≥n de sintaxis

2. **AST** (Prioridad 1)
   - Definir nodos del √°rbol sint√°ctico
   - Component, Decorator, Import, Template, etc.

3. **CSS Generator** (Prioridad 2)
   - Generar CSS desde @utility
   - Resolver tokens tipo Tailwind
   - Optimizaci√≥n y minificaci√≥n

4. **HTML Generator** (Prioridad 2)
   - Generar HTML est√°tico desde templates
   - Interpolaci√≥n de strings simples

5. **Integration** (Prioridad 3)
   - Conectar Lexer ‚Üí Parser ‚Üí Generators
   - Pipeline completo de compilaci√≥n
   - Comando `artic build`

---

## üìö Documentaci√≥n de Referencia

- **GRAMMAR.md** - Gram√°tica formal completa
- **ARCHITECTURE.md** - Arquitectura del compilador
- **MANIFEST.MD** - Especificaci√≥n del lenguaje

---

## üéì Notas de Dise√±o

### Decisiones de Implementaci√≥n

1. **Sin dependencias externas**: El Lexer usa solo la biblioteca est√°ndar de C++
2. **Performance**: Escaneo en un solo paso, sin backtracking
3. **Errores claros**: Cada error incluye posici√≥n exacta
4. **Extensible**: F√°cil agregar nuevos tipos de token

### Limitaciones Conocidas

1. **Template literals**: No soportados a√∫n (`` `hello ${name}` ``)
2. **Regex literals**: No soportados
3. **Unicode**: Soporte b√°sico (no validado completamente)

### Compatibilidad

- Windows ‚úÖ (MSYS2/MinGW)
- Linux ‚úÖ
- macOS ‚úÖ

---

**Versi√≥n del Lexer**: 1.0.0
**Fecha de Completado**: Octubre 29, 2025
**Autor**: Eduardo Alonso (Achronyme)
**Estado**: ‚úÖ PRODUCTION READY

---

## üéâ Conclusi√≥n

El Lexer de Artic est√° completo y funcional. Soporta toda la sintaxis necesaria para la Fase 1 (SSG) y proporciona una base s√≥lida para las siguientes fases del compilador.

**El siguiente paso es implementar el Parser para construir el AST desde los tokens generados por el Lexer.**
